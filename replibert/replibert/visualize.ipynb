{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from wordcloud) (2.1.3)\n",
      "Requirement already satisfied: pillow in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from wordcloud) (11.0.0)\n",
      "Requirement already satisfied: matplotlib in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/imenbenammar/pml-bert/venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/imenbenammar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/imenbenammar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Defining functions to use\n",
    "\n",
    "def load_full_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified path, combines the train, validation, and test splits into a single DataFrame, \n",
    "    shuffles the data, and returns the full dataset.\n",
    "    \n",
    "    Input:\n",
    "    - dataset_path (str): Path to the dataset.\n",
    "    Output:\n",
    "    - full_df (pd.DataFrame): The concatenated and shuffled DataFrame containing the full dataset.\n",
    "    \"\"\"\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    # convert the train, validation, and test splits to a pd dataframe\n",
    "    train_df = dataset['train'].to_pandas()\n",
    "    test_df = dataset['test'].to_pandas()\n",
    "    validation_df = dataset['validation'].to_pandas()   \n",
    "    full_df = pd.concat([train_df, validation_df, test_df], ignore_index=True)\n",
    "    # shuffle\n",
    "    full_df = full_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return full_df\n",
    "    \n",
    "def display_dataset_info(df):\n",
    "    \"\"\"\n",
    "    Displays dataset info and the first 5 rows.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): The dataset.\n",
    "    \n",
    "    Output:\n",
    "    - Prints dataset info and first 5 rows.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Dataset info: \")\n",
    "    df.info()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # display the first 5 rows of the dataset\n",
    "    print(\"\\n First 5 rows of the dataset: \")\n",
    "    print(df.head())\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "def count_nulls(df):\n",
    "    \"\"\"\n",
    "    Counts and returns null values per column.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): The dataset.\n",
    "    \n",
    "    Output:\n",
    "    - null_counts (pd.Series): Count of nulls per column.\n",
    "    \"\"\"\n",
    "    null_counts = df.isnull().sum()\n",
    "    return null_counts\n",
    "\n",
    "def clean_and_tokenize(text, stop_words):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes text, removing non-alphanumeric characters and stopwords.\n",
    "    \n",
    "    Input:\n",
    "    - text (str): The text to clean and tokenize.\n",
    "    - stop_words (list): List of stopwords.\n",
    "    \n",
    "    Output:\n",
    "    - filtered_tokens (list): List of cleaned tokens.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define functions for datasets 1 and 2 \n",
    "\n",
    "def plot_toxicity_subtype_frequency(df, dataset=None):\n",
    "    \"\"\"\n",
    "    Plots the frequency of non-zero values for each toxicity subtype column in the dataset.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): The dataset containing toxicity columns.\n",
    "    - dataset (str): Name of the dataset ('Jigsaw' or 'civil_comments') to select the correct text column.\n",
    "    \n",
    "    Output:\n",
    "    - Displays a bar plot of the frequencies of non-zero values for each toxicity subtype.\n",
    "    \"\"\"\n",
    "    if dataset == 'Jigsaw':\n",
    "        toxicity_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    elif dataset == 'civil_comments':\n",
    "        toxicity_columns = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be either 'Jigsaw' or 'civil_comments'\")\n",
    "    \n",
    "    # list to store the frequency of non-zero values for each column\n",
    "    frequencies = []\n",
    "    # counting non zero values in each toxicity column\n",
    "    for col in toxicity_columns:\n",
    "        non_zero_count = (df[col] > 0).sum() \n",
    "        frequencies.append(non_zero_count)\n",
    "\n",
    "    # plot \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.barplot(x=toxicity_columns, y=frequencies, palette=\"viridis\")\n",
    "    plt.title('Frequency of Each Toxicity Subtype')\n",
    "    plt.xlabel('Toxicity Subtype')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_toxicity_percentage(df, dataset=None):\n",
    "    \"\"\"\n",
    "    Plots the percentage of toxic and non-toxic comments based on the specified toxicity labels.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): The dataset containing toxicity labels.\n",
    "    - dataset (str): Name of the dataset ('Jigsaw' or 'civil_comments') to select the correct text column.\n",
    "    \n",
    "    Output:\n",
    "    - Displays a pie chart showing the percentage of toxic and non-toxic comments.\n",
    "    \"\"\"\n",
    "    if dataset == 'Jigsaw':\n",
    "        toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    elif dataset == 'civil_comments':\n",
    "        toxicity_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be either 'Jigsaw' or 'civil_comments'\")\n",
    "        \n",
    "    # check if any toxicity label is set for each row\n",
    "    df['is_toxic'] = df[toxicity_labels].sum(axis=1) > 0  \n",
    "    # count the number of toxic and non-toxic comments\n",
    "    toxicity_counts = df['is_toxic'].value_counts()\n",
    "    \n",
    "    # prepare data for the pie chart\n",
    "    labels = ['Non-Toxic', 'Toxic']\n",
    "    sizes = [toxicity_counts.get(False, 0), toxicity_counts.get(True, 0)]\n",
    "    explode = (0.1, 0) \n",
    "    # plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.pie(sizes, explode=explode, labels=labels, colors=['#3b528b', '#5ec962'], autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    plt.title('Percentage of Toxic and Non-Toxic Comments')\n",
    "    plt.axis('equal') \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_wordclouds(df, dataset=None):\n",
    "    \"\"\"\n",
    "    Generates and displays word clouds for toxic and non-toxic comments.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): The dataset containing the comments and toxicity labels.\n",
    "    - dataset (str): Name of the dataset ('Jigsaw' or 'civil_comments').\n",
    "    \n",
    "    Output:\n",
    "    - Displays two word clouds, one for toxic and one for non-toxic comments.\n",
    "    \"\"\"\n",
    "    if dataset == 'Jigsaw':\n",
    "        toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    elif dataset == 'civil_comments':\n",
    "        toxicity_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be either 'Jigsaw' or 'civil_comments'\")\n",
    "        \n",
    "    # create a new column 'is_toxic' and set it to true if one toxicity subtype is present\n",
    "    df['is_toxic'] = df[toxicity_labels].sum(axis=1) > 0\n",
    "    # set column name for text\n",
    "    if (dataset == 'Jigsaw'):\n",
    "        text_col = 'comment_text'\n",
    "    elif (dataset == 'civil_comments'):\n",
    "        text_col = 'text'\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be either 'Jigsaw' or 'civil_comments'\")\n",
    "    # separate toxic and non-toxic comments\n",
    "    toxic = ' '.join(df[df['is_toxic']][text_col])\n",
    "    non_toxic = ' '.join(df[~df['is_toxic']][text_col])\n",
    "    \n",
    "    # generate wordclouds\n",
    "    toxic_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds', max_words=100).generate(toxic)\n",
    "    non_toxic_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Greens', max_words=100).generate(non_toxic)\n",
    "    # plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    # toxic comments word cloud\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Word Cloud of Toxic Comments', fontsize=16)\n",
    "    plt.imshow(toxic_wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    # non-toxic comments word cloud\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Word Cloud of Non-Toxic Comments', fontsize=16)\n",
    "    plt.imshow(non_toxic_wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_comment_len(df, dataset=None):\n",
    "    \"\"\"\n",
    "    Analyzes comment lengths and compares them between toxic and non-toxic comments.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing comments and toxicity labels.\n",
    "    - dataset (str): Dataset name ('Jigsaw' or 'civil_comments').\n",
    "    \n",
    "    Output:\n",
    "    - Prints basic statistics and displays plots for comment length distribution, comparison between toxic and non-toxic comments, \n",
    "    and average comment length by toxicity.\n",
    "    \"\"\"\n",
    "    if dataset == 'Jigsaw':\n",
    "        toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        # set column name for text\n",
    "        text_col = 'comment_text'\n",
    "    elif dataset == 'civil_comments':\n",
    "        toxicity_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "        text_col = 'text'\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be either 'Jigsaw' or 'civil_comments'\")\n",
    "        \n",
    "    # create new column and calculate comment lengths\n",
    "    df['comment_length'] = df[text_col].apply(len)\n",
    "    \n",
    "    # display basic statistics of comment lengths\n",
    "    print(\"Basic Statistics for Comment Lengths:\")\n",
    "    print(df['comment_length'].describe())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # plot for distribution of comment lengths\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.histplot(df['comment_length'], bins=20, kde=True, color='#21918c')\n",
    "    plt.title('Distribution of Comment Lengths')\n",
    "    plt.xlabel('Comment Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot to compare comment lengths between Toxic and Non-Toxic comments\n",
    "    df['is_toxic'] = df[toxicity_labels].sum(axis=1) > 0\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.boxplot(data=df, x='is_toxic', y='comment_length', palette='viridis')\n",
    "    plt.xticks([0, 1], ['Non-Toxic', 'Toxic'])\n",
    "    plt.title('Comment Length Comparison: Toxic vs. Non-Toxic')\n",
    "    plt.xlabel('Comment Type')\n",
    "    plt.ylabel('Comment Length')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "  \n",
    "    # plot to visualize the average comment length by toxicity subtype\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    toxicity_avg_length = df.groupby('is_toxic')['comment_length'].mean().reset_index()\n",
    "    sns.barplot(data=toxicity_avg_length, x='is_toxic', y='comment_length', palette='viridis')\n",
    "    plt.xticks([0, 1], ['Non-Toxic', 'Toxic'])\n",
    "    plt.title('Average Comment Length: Toxic vs. Non-Toxic')\n",
    "    plt.xlabel('Comment Type')\n",
    "    plt.ylabel('Average Comment Length')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_word_frequency(df, dataset=None, threshold=0.8, top_n=10):\n",
    "    \"\"\"\n",
    "    Plots the most common words (top N) in toxic comments for each toxicity label above a specified threshold.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing comments and toxicity labels.\n",
    "    - dataset (str): Name of the dataset ('Jigsaw' or 'civil_comments') to select the correct text column.\n",
    "    - threshold (float, optional): Minimum toxicity level to consider a comment as toxic (default is 0.8).\n",
    "    - top_n (int, optional): Number of most common words to display (default is 10).\n",
    "    \n",
    "    Output:\n",
    "    - Displays bar plots for the most common words in toxic comments for each label.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == 'Jigsaw':\n",
    "        toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        # set column name for text\n",
    "        text_col = 'comment_text'\n",
    "    elif dataset == 'civil_comments':\n",
    "        toxicity_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "        text_col = 'text'\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be either 'Jigsaw' or 'civil_comments'\")\n",
    "    \n",
    "    # define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # loop through each toxicity label\n",
    "    for label in toxicity_labels:\n",
    "        # filter rows where the toxicity value for the label is higher than the threshold\n",
    "        is_toxic = df[label] > threshold\n",
    "        toxic_comments = df[is_toxic][text_col]\n",
    "        \n",
    "        # tokenize and clean all comments\n",
    "        all_tokens = []\n",
    "        for comment in toxic_comments:\n",
    "            all_tokens.extend(clean_and_tokenize(comment, stop_words))\n",
    "        \n",
    "        # frequency of each word\n",
    "        word_counts = Counter(all_tokens)\n",
    "        # select top_n most common words\n",
    "        common_words = word_counts.most_common(top_n)\n",
    "        # plot \n",
    "        if common_words:\n",
    "            words, counts = zip(*common_words)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.barplot(x=list(counts), y=list(words), palette='viridis')\n",
    "            plt.title(f'Most Common Words for {label.capitalize()} (Threshold > {threshold})')\n",
    "            plt.xlabel('Frequency')\n",
    "            plt.ylabel('Words')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No words found for {label.capitalize()} with threshold > {threshold}.\")\n",
    "\n",
    "\n",
    "def plot_toxicity_vs_comment_length(df,dataset=None):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot comparing comment length with a specified toxicity label.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing comments and the toxicity labels.\n",
    "    - dataset (str): Name of the dataset ('Jigsaw' or 'civil_comments') to select the correct text column.\n",
    "    \n",
    "    Output:\n",
    "    - Displays a scatter plot of comment length vs the specified toxicity label.\n",
    "    \"\"\"\n",
    "    if dataset == 'Jigsaw':\n",
    "        toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        # set column name for text\n",
    "        text_col = 'comment_text'\n",
    "    elif dataset == 'civil_comments':\n",
    "        toxicity_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "        text_col = 'text'\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be either 'Jigsaw' or 'civil_comments'\")\n",
    "    \n",
    "    \n",
    "    # calculate comment length if not already done\n",
    "    if 'comment_length' not in df.columns:\n",
    "        df['comment_length'] = df[text_col].apply(len)\n",
    "    for label in toxicity_labels:\n",
    "        # plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=df['comment_length'], y=df[label], alpha=0.3, color='#21918c')\n",
    "        plt.title(f'Scatter Plot: Comment Length vs. {label.capitalize()}')\n",
    "        plt.xlabel('Comment Length')\n",
    "        plt.ylabel(f'{label.capitalize()} Level')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define functions for dataset 3\n",
    "\n",
    "def plot_sentiment_frequency_sst2(df):\n",
    "    \"\"\"\n",
    "    Plots the distribution of sentiment labels in the SST-2 dataset.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing a 'label' column with sentiment values (0 for negative, 1 for positive).\n",
    "    \n",
    "    Output:\n",
    "    - Displays a bar plot showing the frequency of positive and negative sentiments.\n",
    "    \"\"\"\n",
    "    sentiment_counts = df['label'].value_counts()\n",
    "    #plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_sentiment_percentage_sst2(df):\n",
    "    \"\"\"\n",
    "    Plots a pie chart showing the percentage of positive and negative sentiments in the SST-2 dataset.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing a 'label' column with sentiment values (0 for negative, 1 for positive).\n",
    "    \n",
    "    Output:\n",
    "    - Displays a pie chart of the sentiment distribution.\n",
    "    \"\"\"\n",
    "    label_counts = df['label'].value_counts()\n",
    "    # map the labels positive and negative\n",
    "    labels = ['Negative', 'Positive']\n",
    "    sizes = label_counts.values\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    explode = (0.1, 0) \n",
    "    plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#3b528b', '#5ec962'])\n",
    "    plt.title('Percentage of Positive and Negative Sentences')\n",
    "    plt.axis('equal') \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_wordclouds_sst2(df):\n",
    "    \"\"\"\n",
    "    Generates and displays word clouds for positive and negative sentences in the SST-2 dataset.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing a 'label' column (0 for negative, 1 for positive) \n",
    "      and a 'sentence' column with text data.\n",
    "    \n",
    "    Output:\n",
    "    - Displays two word clouds, one for positive sentences and one for negative sentences.\n",
    "    \"\"\"\n",
    "    positive_sentence = ' '.join(df[df['label'] == 1]['sentence'])\n",
    "    negative_sentence= ' '.join(df[df['label'] == 0]['sentence'])\n",
    "    negative_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(negative_sentence)\n",
    "    positive_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(positive_sentence)    \n",
    "\n",
    "    # plot \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Word Cloud: Positive Sentences', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "    plt.title('Word Cloud: Negative Sentences', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "def analyze_sentence_len_sst2(df):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes sentence lengths in the SST-2 dataset.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing a 'sentence' column with text data \n",
    "      and a 'label' column (0 for negative, 1 for positive).\n",
    "    \n",
    "    Output:\n",
    "    - Displays basic statistics for sentence lengths, a histogram for their distribution, \n",
    "      and a box plot comparing lengths between positive and negative sentiments.\n",
    "    \"\"\"\n",
    "    # calculate sentence lengths\n",
    "    df['sentence_length'] = df['sentence'].apply(len)\n",
    "    # basic statistics\n",
    "    print(\"Basic Statistics for Sentence Lengths:\")\n",
    "    print(df['sentence_length'].describe())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # plot the distribution of sentence lengths\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(df['sentence_length'], bins=20, kde=True, color='#21918c')\n",
    "    plt.title('Distribution of Sentence Lengths')\n",
    "    plt.xlabel('Sentence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # compare lengths for positive and negative sentiments\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.boxplot(data=df, x='label', y='sentence_length', palette='viridis')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.title('Sentence Length Comparison: Negative vs. Positive')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Sentence Length')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_avg_sentence_length_by_sentiment_sst2(df):\n",
    "    \"\"\"\n",
    "    Plots the average sentence length for negative and positive sentiments in the SST-2 dataset.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing a 'sentence' column with text data \n",
    "      and a 'label' column (0 for negative, 1 for positive).\n",
    "    \n",
    "    Output:\n",
    "    - Displays a bar plot comparing the average sentence length for each sentiment.\n",
    "    \"\"\"\n",
    "    # calculate sentence length \n",
    "    if 'sentence_length' not in df.columns:\n",
    "        df['sentence_length'] = df['sentence'].apply(len)\n",
    "    # average sentence length by sentiment\n",
    "    avg_length = df.groupby('label')['sentence_length'].mean().reset_index()\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.barplot(data=avg_length, x='label', y='sentence_length', palette='viridis')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.title('Average Sentence Length: Negative vs. Positive')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Average Sentence Length')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_word_frequency_sst2(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Plots the most common words in positive or negative sentences from the SST-2 dataset.\n",
    "    \n",
    "    Input:\n",
    "    - df (pd.DataFrame): Dataset containing a 'sentence' column with text data \n",
    "      and a 'label' column (0 for negative, 1 for positive).\n",
    "    - top_n (int, optional): Number of most frequent words to display (default is 10).\n",
    "    \n",
    "    Output:\n",
    "    - Displays a bar plot showing the frequency of the top words for the selected sentiment.\n",
    "    \"\"\"\n",
    "    labels = [0,1]\n",
    "    #define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for label in labels:\n",
    "        # filter sentences by sentiment\n",
    "        text = ' '.join(df[df['label'] == label]['sentence'])\n",
    "        \n",
    "        tokens = clean_and_tokenize(text, stop_words)\n",
    "        # select the most common words\n",
    "        word_counts = Counter(tokens)\n",
    "        common_words = word_counts.most_common(top_n)\n",
    "    \n",
    "        # plot\n",
    "        words, counts = zip(*common_words)\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        sns.barplot(x=list(counts), y=list(words), palette='viridis')\n",
    "        plt.title(f'Most Common Words in {\"Positive\" if label == 1 else \"Negative\"} Sentences')\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Words')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show(plot_type, outfile=None, dataset=None):\n",
    "    \"\"\"\n",
    "    Dispatches visualization functions based on `plot_type` and dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - plot_type (str): The type of plot to generate.\n",
    "    - outfile (str, optional): Path to save the plot (default is None).\n",
    "    - dataset (str, optional): Specifies the dataset (e.g., \"Jigsaw\", \"civil_comments\", or \"SST2\").\n",
    "\n",
    "    Output:\n",
    "    - Displays the plot and optionally saves it to a file.\n",
    "    \"\"\"\n",
    "    # load the corresponding dataset\n",
    "    if dataset == \"Jigsaw\":\n",
    "        try:\n",
    "            toxicity_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "            train_df = pd.read_csv('../data/jigsaw_toxicity_pred/train.csv')\n",
    "            initial_test_df = pd.read_csv('../data/jigsaw_toxicity_pred/test.csv')\n",
    "            test_labels_df = pd.read_csv('../data/jigsaw_toxicity_pred/test_labels.csv')\n",
    "            \n",
    "            # filter out rows with toxicity labels = -1\n",
    "            valid_labels_df = test_labels_df[~test_labels_df[toxicity_labels].eq(-1).any(axis=1)]\n",
    "            # merge test data with the filtered valid labels only\n",
    "            test_df = pd.merge(initial_test_df, valid_labels_df, on=\"id\", how=\"inner\")\n",
    "            # combine both train and test sets\n",
    "            df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading dataset: {e}\")\n",
    "            \n",
    "        display_dataset_info(df)\n",
    "\n",
    "    elif dataset == \"civil_comments\":\n",
    "        try:\n",
    "            toxicity_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "            df = load_full_dataset(\"../data/finetuning_datasets/civil_comments\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading dataset: {e}\")\n",
    "        \n",
    "        display_dataset_info(df)\n",
    "        \n",
    "    elif dataset == \"SST2\":\n",
    "        try:\n",
    "            df = load_full_dataset(\"../data/finetuning_datasets/sst2\")\n",
    "            \n",
    "            # determine the number of hidden labels (-1)\n",
    "            hidden_label_count = (df['label'] == -1).sum()\n",
    "            print(f\"Number of hidden labels in the dataset: {hidden_label_count}\")\n",
    "            # filter out hidden labels\n",
    "            df = df[df['label'] != -1]\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading dataset: {e}\")\n",
    "            \n",
    "        display_dataset_info(df)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset '{dataset}'. Available datasets: Jigsaw, civil_comments, or SST2.\")\n",
    "        \n",
    "\n",
    "    # check for null values\n",
    "    count_nulls(df)\n",
    "    \n",
    "    # assign function mappings for the datasets\n",
    "    jigsaw_cc_plots = {\n",
    "        \"toxicity_frequency\": plot_toxicity_subtype_frequency,\n",
    "        \"toxicity_percentage\": plot_toxicity_percentage,\n",
    "        \"wordclouds\": plot_wordclouds,\n",
    "        \"comment_length_analysis\": analyze_comment_len,\n",
    "        \"toxicity_vs_length\": plot_toxicity_vs_comment_length,\n",
    "        \"word_frequency\": plot_word_frequency,\n",
    "        \n",
    "    }\n",
    "\n",
    "    sst2_plots = {\n",
    "        \"sentiment_frequency\": plot_sentiment_frequency_sst2,\n",
    "        \"sentiment_percentage\": plot_sentiment_percentage_sst2,\n",
    "        \"wordclouds\": plot_wordclouds_sst2,\n",
    "        \"sentence_length_analysis\": analyze_sentence_len_sst2,\n",
    "        \"avg_sentence_length\": plot_avg_sentence_length_by_sentiment_sst2,\n",
    "        \"word_frequency\": plot_word_frequency_sst2,\n",
    "    }\n",
    "\n",
    "    # select the corresponding function dictionary\n",
    "    plot_functions = {}\n",
    "    if dataset == \"Jigsaw\" or dataset == 'civil_comments':\n",
    "        plot_functions = jigsaw_cc_plots\n",
    "    elif dataset == \"SST2\":\n",
    "        plot_functions = sst2_plots\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset '{dataset}'. Available datasets: Jigsaw, Civil_comments, or SST2.\")\n",
    "\n",
    "    # validate the plot_type\n",
    "    if plot_type not in plot_functions:\n",
    "        available_types = list(plot_functions.keys())\n",
    "        raise ValueError(f\"Unsupported plot_type '{plot_type}'. Available types: {available_types}\")\n",
    "\n",
    "    # call the plot function\n",
    "    plot_func = plot_functions[plot_type]\n",
    "    if dataset == \"SST2\":\n",
    "        plot_func(df)\n",
    "    else:\n",
    "        plot_func(df, dataset)\n",
    "\n",
    "    # save the plot if outfile is provided\n",
    "    if outfile:\n",
    "        plt.savefig(outfile, dpi=kwargs.get(\"dpi\", 300))\n",
    "        print(f\"Visualization saved to {outfile}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Examples\n",
    "# Note: Runninf the following lines will print the dataset info many times, as the show() function is intented to be run only once for \n",
    "# a plot at a time\n",
    "\n",
    "show('toxicity_frequency', outfile=None, dataset=\"Jigsaw\")\n",
    "show('toxicity_frequency', outfile=None, dataset=\"civil_comments\")\n",
    "#show('toxicity_percentage', outfile=None, dataset=\"Jigsaw\")\n",
    "#show('wordclouds', outfile=None, dataset=\"Jigsaw\")\n",
    "#show('comment_length_analysis', outfile=None, dataset=\"Jigsaw\")\n",
    "#show('toxicity_vs_length', outfile=None, dataset=\"Jigsaw\")\n",
    "#show('word_frequency', outfile=None, dataset=\"Jigsaw\")\n",
    "\n",
    "show('sentiment_frequency', outfile=None, dataset=\"SST2\")\n",
    "#show('sentiment_percentage', outfile=None, dataset=\"SST2\")\n",
    "#show('word_frequency', outfile=None, dataset=\"SST2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}