data:
  train:
    - identifier: 'wikimedia/wikipedia'
      short: 'wikipedia'
      name: '20231101.en'
      split: 'train'
    - identifier: 'manu/project_gutenberg'
      short: 'project_gutenberg'
      name: null
      split: 'en'
  finetuning:
    - identifier: 'google/civil_comments'
      short: 'civil_comments'
      name: null
      split: null
    - identifier: 'google/jigsaw_toxicity_pred'
      short: 'jigsaw_toxicity_pred'
      name: null
      split: null
      config_kwargs:
        data_dir: 'data/jigsaw_toxicity_pred'
    - identifier: 'stanfordnlp/sst2'
      short: 'sst2'
      name: null
      split: null

model:
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  max_position_embeddings: 256
  vocab_size: 30522

finetuning:
  dataset_fraction: 1.0
  batch_size: 16
  learning_rate: 5e-5
  weight_decay: 0.01
  num_epochs: 1
  device: "cuda"
