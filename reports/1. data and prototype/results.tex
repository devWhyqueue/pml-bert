\section{Results}
This section presents the evaluation of our baselines' performances on the Civil Comments data set. The evaluation is conducted using multiple standard metrics to assess its effectiveness in classifying toxic and non-toxic comments. Note that this dataset poses a binary classification task. Therefore, category differences were no issue, and adaption of classic binary validation metrics was not necessary.

We include the metrics accuracy, precision, recall, F1-score, and area under the ROC curve (AUC-ROC). These metrics are widely used in the literature, allowing us to ensure that our results are comparable with existing studies. \cite{Androcec2020, Duchene2023, Gladwin2022} 

The results highlight the method's strengths and provide insights into areas requiring further improvement.

\subsection{Evaluation metrics}
The metrics are defined as follows.

\textbf{Accuracy} measures the proportion of all classification instances that are classified correctly:

\begin{equation}
    \mathrm{Accuracy} = \frac{\mathrm{TP} + \mathrm{TN}}{\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}},
\end{equation}

Where $\mathrm{TP}$, $\mathrm{TN}$, $\mathrm{FP}$, and $\mathrm{FN}$ represent the number of true positives, true negatives, false positives, and false negatives, respectively.

Precision indicates the proportion of predicted positive instances that are positive:

\begin{equation}
    \mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}.
\end{equation}

\textbf{Recall} measures the amount of correctly positive classified instances proportionate to the true amount of positive classifications:

\begin{equation}
    \mathrm{Recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}.
\end{equation}

\textbf{F1-score} represents the harmonic mean of precision and recall:

\begin{equation}
    F_1 = 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}.
\end{equation}

Finally, the area under the receiver operating characteristic curve (\textbf{AUC-ROC}) measures the ability of the model to distinguish between classes across all classification thresholds. It is computed as:

\begin{equation}
    \mathrm{AUC\text{-}ROC} = \int_{0}^{1} \mathrm{TPR}(\mathrm{FPR}) \, d(\mathrm{FPR}),
\end{equation}

where $\mathrm{TPR}$ (true positive rate) and $\mathrm{FPR}$ (false positive rate) are defined as:

\begin{equation}
        \mathrm{TPR} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}, \quad \mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}}.
\end{equation}

We evaluate the performances using precision, recall, and F1-score specifically for the toxic class (Class 1) because it represents the primary focus in toxic comment classification. The ability to correctly identify and classify toxic comments is critical, as these instances typically have a disproportionate impact on user experience and platform safety. By focusing on these metrics for Class 1, we ensure the evaluation reflects the model's effectiveness in addressing the key challenge of identifying toxicity.

Additionally, we include macro-averaged F1 and weighted-averaged F1 scores to provide a holistic evaluation across all comment classes. The macro-averaged F1 treats all classes equally, offering insights into how well the model performs across all types of comments, regardless of class imbalance. The weighted-averaged F1 adjusts for class distribution, ensuring that the evaluation accounts for the prevalence of non-toxic comments, which are the majority class.

Finally, overall accuracy is reported as a general performance metric for completeness, though its limitations in imbalanced datasets necessitate reliance on the other metrics for a more comprehensive understanding of the model's ability to detect toxic comments.

\subsection{Findings}

Table \ref{tab:performance} shows the results of the grid search over baseline methods and hyper-parameters pre-processing and proportion of positive samples. They reveal important insights into the performance of the classifiers and the challenges inherent in the task.

\begin{table}[h!]
\centering
\caption{ Classifier performance on the Civil Comments validation dataset}
\label{tab:performance}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\hline
\textbf{Method}            & \textbf{Prep.} & \textbf{Pos.} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Macro} & \textbf{Weighted} & \textbf{Acc.} & \textbf{AUC} \\ 
                           &                        & \textbf{Prop.} & \textbf{(C1)}      & \textbf{(C1)}   & \textbf{(C1)} & \textbf{F1}   & \textbf{F1}       &               &      \textbf{ROC}            \\ \hline
Na√Øve Bayes                & True                   & 0.1           & \textbf{0.86}               & 0.15            & 0.25         & 0.61          & 0.93             & 0.95         & 0.88             \\ 
                           &                        & 0.25          & 0.58               & 0.39            & 0.47         & 0.72          & 0.94             & 0.95         & 0.88             \\ 
                           &                        & 0.5           & 0.14               & \textbf{0.85}            & 0.25         & 0.53          & 0.77             & 0.69         & 0.86             \\ 
                           & False                  & 0.1           & 0.83               & 0.18            & 0.29         & 0.63          & 0.93             & 0.95         & 0.87             \\ 
                           &                        & 0.25          & 0.52               & 0.44            & 0.48         & 0.73          & 0.94             & 0.94         & 0.87             \\ 
                           &                        & 0.5           & 0.14                 & \textbf{0.85}              & 0.24           & 0.52            & 0.76               & 0.68           & 0.86               \\ \hline
Support Vector     & True                   & 0.1           & 0.52               & 0.60            & 0.56         & 0.76          & 0.95             & 0.94         & 0.89             \\ 
Machine                           &                        & 0.25          & 0.37               & 0.72            & 0.49         & 0.72          & 0.92             & 0.91         & 0.90             \\ 
                           &                        & 0.5           & 0.21                 & 0.82              & 0.34           & 0.61            & 0.86               & 0.81           & 0.89               \\ 
                           & False                  & 0.1           & 0.51               & 0.59            & 0.55         & 0.76          & 0.94             & 0.94         & 0.88             \\ 
                           &                        & 0.25          & 0.38                 & 0.68              & 0.49           & 0.72            & 0.93               & 0.92           & 0.89               \\ 
                           &                        & 0.5           & 0.25                 & 0.76              & 0.37           & 0.64            & 0.88               & 0.85           & 0.88               \\ \hline
Logistic        & True                   & 0.1           & 0.67               & 0.43            & 0.52         & \textbf{0.75}          & \textbf{0.95}             & \textbf{0.95}         & \textbf{0.90}             \\ 
Regression                           &                        & 0.25          & 0.50               & 0.55            & \textbf{0.53}         & \textbf{0.75}          & 0.94             & 0.94         & \textbf{0.90}             \\ 
                           &                        & 0.5           & 0.22                 & 0.78              & 0.34           & 0.62            & 0.87               & 0.82           & 0.88               \\ 
                           & False                  & 0.1           & 0.70                 & 0.37              & 0.48           & 0.73            & 0.95               & 0.99           & 0.89               \\ 
                           &                        & 0.25          & 0.56                 & 0.47              & 0.51           & 0.74            & 0.94               & 0.95           & 0.89               \\ 
                           &                        & 0.5           & 0.27                 & 0.68              & 0.39           & 0.66            & 0.90               & 0.87           & 0.87               \\ \hline
Random Forest              & True                   & 0.1           & 0.47               & 0.61            & \textbf{0.53}         & \textbf{0.75}          & 0.94             & 0.94         & 0.88             \\ 
                           &                        & 0.25          & 0.37                 & 0.73              & 0.49           & 0.72            & 0.92               & 0.91           & 0.90               \\ 
                           &                        & 0.5           & 0.22                 & 0.81              & 0.35           & 0.62            & 0.86               & 0.82           & 0.89               \\ 
                           & False                  & 0.1           & 0.45                 & 0.61              & 0.52           & 0.74            & 0.94               & 0.93           & 0.87               \\ 
                           &                        & 0.25          & 0.37                 & 0.69              & 0.49           & 0.72            & 0.92               & 0.91           & 0.88               \\ 
                           &                        & 0.5           & 0.22                 & 0.75              & 0.34           & 0.62            & 0.87               & 0.83           & 0.85               \\ \hline
\end{tabular}%
}
\end{table}

Logistic Regression, when combined with preprocessing and evaluated on positive sample proportions of 0.1 and 0.25, demonstrated the best performance among all tested methods. These configurations achieved the highest overall scores across critical metrics, including weighted F1, accuracy, and AUC-ROC. The evaluation on the test set confirmed these findings, with all three metrics reaching satisfactory levels ($\geq 0.9$), indicating robust performance in distinguishing toxic and non-toxic comments. This consistency suggests that overfitting is not a significant concern in these configurations, further reinforcing their reliability and generalizability.

Despite the overall strong performance, the precision, recall, and F1 scores for the positive class (C1) remained suboptimal, peaking at only 0.53. This limitation suggests that the model struggles with effectively identifying and classifying toxic comments. The implications of these results highlight the need for further investigation into strategies to improve the model's sensitivity to toxic content, such as better handling of class imbalances, enhancing feature representation, or exploring alternative architectures.

Preprocessing had no significant or consistent impact across all metrics. While it slightly enhanced performance in certain configurations, the benefit was not universally observed across all classifiers or dataset splits. This suggests that preprocessing alone is insufficient to address the challenges posed by the dataset's inherent characteristics, such as class imbalance or feature sparsity.

The imbalance of the dataset emerged as the most critical factor influencing classifier performance. Higher positive proportions led to noticeable declines in precision and overall F1 scores, underscoring the challenges of effectively handling the minority toxic class. These results emphasize the importance of addressing imbalance, potentially through oversampling, undersampling, or class-weighted learning techniques, to achieve more reliable toxic comment classification.
