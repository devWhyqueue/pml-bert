\section{Introduction}\label{sec:intro}

The previous milestone established baseline benchmarks for the task of toxic comment classification by implementing classical ML methods. These methods included logistic regression, SVM, and naive Bayes, evaluated on the Civil Comments dataset. While the best-performing baseline model achieved a peak F1-score of 0.62 for the toxic class, the overall performance was deemed unsatisfactory.\footnote{The results reported in the previous milestone contained errors. A detailed explanation and the corrected results can be found on GitHub.}
This result underscores the limitations of classical ML methods in handling the inherent complexities of the toxic comment classification task, particularly in the presence of significant class imbalance. A key finding of the previous milestone was that class imbalance adversely affected the precision and recall of the minority toxic class (C1). Addressing this issue remains critical as we transition to more advanced approaches, such as fine-tuning the BERT model.

The goal of this milestone is to evaluate the performance of fine-tuned transformer-based algorithms, specifically BERT, on the toxic comment classification task. By leveraging BERT's deep contextual understanding and pre-training on extensive text corpora, we aim to overcome the shortcomings observed with classical ML approaches. Furthermore, we will examine whether the class imbalance challenge persists when using more sophisticated models and explore strategies for mitigating its impact.
\begin{comment}
    This report is structured as follows. \Cref{sec:meth} provides a concise description of the methods employed, including data preprocessing, model selection, and hyperparameter optimization. \Cref{sec:generalization} discusses generalization, showcasing some best and worst case predictions. \Cref{sec:results} presents the experimental results, including detailed performance metrics, error analysis, and comparisons to baseline methods. Finally, \cref{sec:discussion} evaluates the computational efficiency of the fine-tuning process, explores confidence measures for predictions, and assesses the overall suitability of BERT for this application.

Through this milestone, we aim to establish a robust evaluation of BERT's capabilities and its advantages over classical baselines, paving the way for further refinements in toxic comment classification.
\end{comment}
